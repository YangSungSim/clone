import numpy as np
import pandas as pd
import datetime
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

train = pd.read_csv('../Documents/train_final.csv')
test = pd.read_csv('../Documents/test_final.csv')
#pd.options.display.max_columns = 30

#대략 정보 확인
train.info()
train.describe()

#null값 확인
#np.count_nonzero(train.isnull()) 갯수 300 개. 2000개 열 중에 이걸 다 날리면...
train.dropna(axis=0, inplace=True) #100여개 되는 열을 싹 다 날림
test.dropna(axis=0, inplace=True)

#####################################################
'''
#시각데이터로 뭐가 어떻게 생겼는지 보기
plt.figure()
plt.title("counts per hour")
plt.xlabel("time of day")
plt.ylabel("count")
plt.plot(list(range(24)),[sum(train[train.hour==i]['count'])/max(1,len(train[train.hour==i]['count'])) for i in range(24)])

binwidth = (max(train['hour_bef_temperature'])-min(train['hour_bef_temperature']))/10
bins = np.arange(min(train['hour_bef_temperature']),max(train['hour_bef_temperature']),binwidth)
train['cutted'] = pd.cut(train['hour_bef_temperature'],bins,labels=list(range(9)))

train.loc[:,['cutted','count']].groupby('cutted').mean().plot()

plt.plot(["0","1"],[len(train['hour_bef_precipitation'][train.hour_bef_precipitation==0]), len(train['hour_bef_precipitation'][train.hour_bef_precipitation==1])])



binwidth = (max(train['hour_bef_windspeed'])-min(train['hour_bef_windspeed']))/10
bins = np.arange(min(train['hour_bef_windspeed']),max(train['hour_bef_windspeed']),binwidth)
train['cutted2'] = pd.cut(train['hour_bef_windspeed'],bins,labels=list(range(9)))

train.loc[:,['cutted2','count']].groupby('cutted2').mean().plot()



binwidth = (max(train['hour_bef_humidity'])-min(train['hour_bef_humidity']))/10
bins = np.arange(min(train['hour_bef_humidity']),max(train['hour_bef_humidity']),binwidth)
train['cutted3'] = pd.cut(train['hour_bef_humidity'],bins,labels=list(range(9)))

train.loc[:,['cutted3','count']].groupby('cutted3').mean().plot()

import seaborn as sns
sns.heatmap(train.iloc[:,1:-3].corr())
'''
###########################################################


# data와 target 분리 시키기
data = train.iloc[:,1:-1].values
target = train.iloc[:,-1].values
#데이터 정규화
scalar = MinMaxScaler()
scalar.fit_transform(data)


#test 와 validation set을 나누기
X_train,X_val,y_train,y_val = train_test_split(data,target,\
                                               random_state=1)

#모델 생성
# 전부다 숫자로 된 데이터고 또 숫자를 예측해야 하니까.. logistic regression
import xgboost
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
import sklearn.linear_model #2개의 feature가 중요하고 데이터가 100만개 이하기 때문에 선택

'''
param_grid = {"objective":["reg:squarederror"],
              "max_depth":[6,10],
              "n_estimators":[5000,10000],
              "subsample":[0.5],
              "gamma":[0.1,1,10,100],
              "reg_lambda":[100],
              "reg_alpha":[10]}


Grid = GridSearchCV(xgboost.XGBRegressor(), param_grid,cv=5, return_train_score=True, iid=True)

Grid.fit(X_train,y_train)

Grid.best_params_
'''

model = xgboost.XGBRegressor(objective='reg:squarederror',
                             gamma=10,max_depth=6,n_estimators=5000,
                             reg_alpha=10,reg_lambda=100,subsample=0.5)
model.fit(X_train,y_train)

print("train의 점수:",model.score(X_train,y_train))
print("test의 점수:",model.score(X_val,y_val))

'''
for i in [0.0001,0.001,0.01,0.1,1,10,100]:
    model1 = sklearn.linear_model.Ridge(alpha=i,max_iter=100000)
    model1.fit(X_train,y_train)

    print("train의 점수:",model1.score(X_train,y_train))
    print("test의 점수:",model1.score(X_val,y_val))
'''

#실행
model.fit(data,target)

y_pred = model.predict(X_train)
mse = mean_squared_error(y_pred,y_train)
print("학습오차:",mse)



'''
#test의 값으로 예상치 뽑아내기
pred = model.predict(test.iloc[:,1:].values)
test['count'] = pred
test['count'] = test['count'].apply(int)
submission = pd.concat([test['id'],test['count']],axis=1)
submission.to_csv('../Documents/submission.csv')
'''




